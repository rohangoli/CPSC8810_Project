{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8feb688-818a-4cb4-be24-712f2e384245",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"./source.txt\"\n",
    "def download_dataset(url):\n",
    "    with open(\"./source.txt\",'r') as fh:\n",
    "        source_files_list = list(line.strip() for line in fh.readlines())\n",
    "        for file_ in source_files_list:\n",
    "            subprocess.check_output(f\"wget {file_}\",shell=True)\n",
    "\n",
    "DATADIR = \"./aws_customer_reviews/\"\n",
    "def unzip_files(DATADIR):\n",
    "    gz_files = os.listdir(DATADIR)\n",
    "    source_files_list = list(line.strip() for line in gz_files)\n",
    "    for file_ in source_files_list:\n",
    "    #     print(file_)\n",
    "        print(subprocess.call([\"gzip\",\"-d\",os.path.join(DATADIR,file_)],shell=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df3d537-b3c7-4b96-9784-2412c53abe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/software/spackages/linux-centos8-x86_64/gcc-8.3.1/spark-3.1.2-fbpdjvdj5fjlkjnu35rh4aooi2mirndz\n",
      "/home/rgadde/ondemand/data/sys/dashboard/batch_connect/sys/palmetto_jupyter_spark/output/0207c552-379e-4eba-a592-a0321647ac1f/spark-defaults.conf\n",
      "/software/spackages/linux-centos8-x86_64/gcc-8.3.1/spark-3.1.2-fbpdjvdj5fjlkjnu35rh4aooi2mirndz\n",
      "node0122.palmetto.clemson.edu\n",
      "30758\n",
      "20605\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pyspark\n",
    "import subprocess\n",
    "print(os.environ['SPARK_ROOT'])\n",
    "print(os.environ['SPARK_CONFIG_FILE'])\n",
    "print(os.environ['SPARK_ROOT'])\n",
    "print(os.environ['SPARK_MASTER_HOST'])\n",
    "print(os.environ['SPARK_MASTER_PORT'])\n",
    "print(os.environ['SPARK_MASTER_WEBUI_PORT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f51f74-d5dc-463a-adab-8b0001f1cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "PUNCTUATIONS = \"\"\"[!\"#$%&()*+-.\\/\\\\:;<=>?@\\[\\]^_`{|}\\t\\n\\',~â€”]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1253d0-13d8-4ccb-9f16-bd82e4221ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "import gzip\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f5dacb-8c55-45d6-9f05-ec1e5ac9dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f77e1c-b85e-4c33-9d61-ea5298272e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e28367-2968-450d-b623-89e328a37f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from timeit import default_timer as timer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65da31b6-2183-4434-ae44-72d0795ec9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fbb1b8e-3ef6-42c5-a470-ed3f36b924f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"./aws_customer_reviews/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be73788a-8dce-4145-951e-309c4d587099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_gunzip(path):\n",
    "#     with gzip.open(path,'rt') as fh:\n",
    "    data = spark.read.csv(path,sep=\"\\t\")\n",
    "    return data\n",
    "def read_tsv(path):\n",
    "#     print(path)\n",
    "    data = spark.read.csv(path,sep=\"\\t\",header = True)\n",
    "    data = data.select([\"review_body\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4975342-70d9-4e81-8a6d-40725983deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = list(filter(lambda x:x.endswith(\".tsv\"),os.listdir(DATADIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94bd6a79-8f0b-42df-891d-3c7dbb021baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_objects = [file_.split(\"_\")[3] for file_ in data_files]\n",
    "data_files_parallelized = sc.parallelize([os.path.join(DATADIR,file_) for file_ in data_files[0:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "272d4e7d-d1c6-44c7-9781-65b115158fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./aws_customer_reviews/amazon_reviews_us_Books_v1_00.tsv']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files_parallelized.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49196203-51b2-4666-9ab5-751c6a5a9fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read time:  3.362594846636057\n"
     ]
    }
   ],
   "source": [
    "read_t1 = timer()\n",
    "dataframes = list(map(lambda file_:read_tsv(file_),data_files_parallelized.collect()))\n",
    "read_t2 = timer()\n",
    "print(\"Read time: \",read_t2 - read_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8801ed40-5402-4432-b156-6b14c3c159c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for file_ in data_files:\n",
    "    size += os.path.getsize(os.path.join(DATADIR,file_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc88462c-3953-4c38-b4d7-381bc066bdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.94232512107001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size *9.31*1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce1feb22-1628-4ec4-995f-c78e4c048c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyschema = StructType([\n",
    "  StructField('review_body', StringType(), True)\n",
    "  ])\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "full_data_frame = spark.createDataFrame(emptyRDD,emptyschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12703de5-0e03-4a95-b40b-aed421e6002b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_body: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframes[0].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd24ec4c-135c-4a43-bbfa-f305ff22082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dataframes:\n",
    "    full_data_frame = full_data_frame.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70ba47cc-52df-4b3a-8b64-45a913e240e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = \"review_body\"\n",
    "\n",
    "def tf_idf_pre_processing(dataframe,column_name):\n",
    "    dataframe = dataframe.na.drop(subset=[column_name])\n",
    "    dataframe = dataframe.withColumn(column_name,f.lower(f.col(column_name)))\n",
    "    dataframe = dataframe.withColumn(column_name,f.regexp_replace(f.col(column_name), PUNCTUATIONS, ' '))\n",
    "    dataframe = dataframe.withColumn(column_name,f.trim(f.col(column_name)))\n",
    "    tokenizer = Tokenizer(inputCol=column_name, outputCol=\"words\")\n",
    "    dataframe = tokenizer.transform(dataframe)                  \n",
    "    stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=STOPWORDS)\n",
    "    data = stopwordsRemover.transform(dataframe)\n",
    "    return data\n",
    "\n",
    "def hash_tf(dataframe):\n",
    "    starttime = timer()\n",
    "    tokenizer_hastingtf = Tokenizer(inputCol=\"review_body\", outputCol=\"tokenized\")\n",
    "    tokenized_data = tokenizer_hastingtf.transform(dataframe)\n",
    "    hashingTF = HashingTF(inputCol=\"tokenized\", outputCol=\"rawFeatures\")\n",
    "    result = hashingTF.transform(tokenized_data)\n",
    "    print(\"Time to Hash\", timer() - starttime)\n",
    "    return result\n",
    "\n",
    "def count_tf(dataframe):\n",
    "    data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "    countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\", vocabSize=100000, minDF=5)\n",
    "    model = countVectors.fit(data)\n",
    "    result = model.transform(data)\n",
    "    return result\n",
    "\n",
    "def idf_generator(dataframe):\n",
    "    starttime = timer()\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfmodel = idf.fit(dataframe)\n",
    "    idf_data = idfmodel.transform(dataframe)\n",
    "    print(\"Time to idf\", timer() - starttime)\n",
    "    return idf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239aa4e-93d7-4689-970b-b912983aa0c4",
   "metadata": {},
   "source": [
    "## TF IDF with HasingTF and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f2b9d-0970-45a6-8ff3-dd3396ae0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "starttime = timer()\n",
    "result = hash_tf(data)\n",
    "idf_data = idf_generator(result)\n",
    "print(\"Time to HashingTFIDF:\", timer() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20edb804-1ade-4456-a26c-27c73a2d3fde",
   "metadata": {},
   "source": [
    "## TF IDF with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9249f0fe-b181-4d9b-8460-1f6580c95cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to CountVectorizer 127.11185103282332\n",
      "Time to IDF 117.03020508214831\n"
     ]
    }
   ],
   "source": [
    "starttime = timer()\n",
    "result = count_tf(data)\n",
    "idf_data = idf_generator(result)\n",
    "print(\"Time to CountVectorizer\", timer() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebf74d-e1d6-4131-a796-8e5becf2feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Rows in TFIDF dataframes: \", idf_data.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
