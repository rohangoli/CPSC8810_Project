{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8138f-d758-499f-8d75-3bc5dbc00702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8feb688-818a-4cb4-be24-712f2e384245",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"./source.txt\"\n",
    "\n",
    "def list_urls(file):\n",
    "\n",
    "    with open(\"./source.txt\",'r') as fh:\n",
    "        source_files_list = list(line.strip() for line in fh.readlines())\n",
    "    return source_files_list\n",
    "    \n",
    "    \n",
    "def download_dataset(url):\n",
    "    try:\n",
    "        subprocess.call(f\"wget {url}\",shell=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "DATADIR = \"/scratch1/rgoli/aws_customer_reviews\"\n",
    "def unzip_files(DATADIR):\n",
    "    gz_files = os.listdir(DATADIR)\n",
    "    source_files_list = list(line.strip() for line in gz_files)\n",
    "    for file_ in source_files_list:\n",
    "    #     print(file_)\n",
    "        print(subprocess.call([\"gzip\",\"-d\",os.path.join(DATADIR,file_)],shell=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498068f-e448-427d-a05b-4871e5ef5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files_list = list(filter(lambda file_:file_.endswith(\".tsv\"),os.listdir(DATADIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3d537-b3c7-4b96-9784-2412c53abe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "print(os.environ['SPARK_ROOT'])\n",
    "print(os.environ['SPARK_CONFIG_FILE'])\n",
    "print(os.environ['SPARK_ROOT'])\n",
    "print(os.environ['SPARK_MASTER_HOST'])\n",
    "print(os.environ['SPARK_MASTER_PORT'])\n",
    "print(os.environ['SPARK_MASTER_WEBUI_PORT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f51f74-d5dc-463a-adab-8b0001f1cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "PUNCTUATIONS = \"\"\"[!\"#$%&()*+-.\\/\\\\:;<=>?@\\[\\]^_`{|}\\t\\n\\',~â€”]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1253d0-13d8-4ccb-9f16-bd82e4221ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "import gzip\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5dacb-8c55-45d6-9f05-ec1e5ac9dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f77e1c-b85e-4c33-9d61-ea5298272e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e28367-2968-450d-b623-89e328a37f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from timeit import default_timer as timer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da31b6-2183-4434-ae44-72d0795ec9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73788a-8dce-4145-951e-309c4d587099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_gunzip(path):\n",
    "#     with gzip.open(path,'rt') as fh:\n",
    "    data = spark.read.csv(path,sep=\"\\t\")\n",
    "    return data\n",
    "def read_tsv(path):\n",
    "#     print(path)\n",
    "    data = spark.read.csv(path,sep=\"\\t\",header = True)\n",
    "    data = data.select([\"review_body\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4975342-70d9-4e81-8a6d-40725983deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = list(filter(lambda x:x.endswith(\".tsv\"),os.listdir(DATADIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd6a79-8f0b-42df-891d-3c7dbb021baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_objects = [file_.split(\"_\")[3] for file_ in data_files]\n",
    "data_files_parallelized = sc.parallelize([os.path.join(DATADIR,file_) for file_ in data_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d4e7d-d1c6-44c7-9781-65b115158fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files_parallelized_list = data_files_parallelized.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49196203-51b2-4666-9ab5-751c6a5a9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_t1 = timer()\n",
    "dataframes = list(map(lambda file_:read_tsv(file_),data_files_parallelized_list))\n",
    "read_t2 = timer()\n",
    "print(\"Read time: \",read_t2 - read_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801ed40-5402-4432-b156-6b14c3c159c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for file_ in data_files:\n",
    "    size += round(os.path.getsize(os.path.join(DATADIR,file_))*9.31*1e-10,2)\n",
    "    print(round(size,2),int(int(size) % 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c73e14-be6d-4000-8253-028d8c8a7fd7",
   "metadata": {},
   "source": [
    "## ALL Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba47cc-52df-4b3a-8b64-45a913e240e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tf_idf_pre_processing(dataframe,column_name):\n",
    "    dataframe = dataframe.na.drop(subset=[column_name])\n",
    "    dataframe = dataframe.withColumn(column_name,f.lower(f.col(column_name)))\n",
    "    dataframe = dataframe.withColumn(column_name,f.regexp_replace(f.col(column_name), PUNCTUATIONS, ' '))\n",
    "    dataframe = dataframe.withColumn(column_name,f.trim(f.col(column_name)))\n",
    "    tokenizer = Tokenizer(inputCol=column_name, outputCol=\"words\")\n",
    "    dataframe = tokenizer.transform(dataframe)                  \n",
    "    stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=STOPWORDS)\n",
    "    data = stopwordsRemover.transform(dataframe)\n",
    "    return data\n",
    "\n",
    "def hash_tf(dataframe):\n",
    "    starttime = timer()\n",
    "#     tokenizer_hastingtf = Tokenizer(inputCol=\"review_body\", outputCol=\"tokenized\")\n",
    "#     tokenized_data = tokenizer_hastingtf.transform(dataframe)\n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "    result = hashingTF.transform(dataframe)\n",
    "    print(\"Time to Hash\", timer() - starttime)\n",
    "    return result\n",
    "\n",
    "def count_tf(dataframe):\n",
    "    countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\", vocabSize=100000, minDF=5)\n",
    "    model = countVectors.fit(dataframe)\n",
    "    result = model.transform(dataframe)\n",
    "    return result\n",
    "\n",
    "\n",
    "def idf_generator(dataframe):\n",
    "    starttime = timer()\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfmodel = idf.fit(dataframe)\n",
    "    idf_data = idfmodel.transform(dataframe)\n",
    "    print(\"Time to idf\", timer() - starttime)\n",
    "    return idf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239aa4e-93d7-4689-970b-b912983aa0c4",
   "metadata": {},
   "source": [
    "# TF IDF with HasingTF and IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50d344-bd58-4ca8-94c7-054ea684652f",
   "metadata": {},
   "source": [
    "## Performance data size vs Computation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfea69-5ff7-4da9-9f01-eb87c3c70908",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyschema = StructType([\n",
    "  StructField('review_body', StringType(), True)\n",
    "  ])\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "full_data_frame = spark.createDataFrame(emptyRDD,emptyschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f2632-095f-4c99-bd17-9207a017037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = \"review_body\"\n",
    "while True:\n",
    "    size = 0\n",
    "    time_dict = {}\n",
    "    i=0\n",
    "    for file_, df in list(zip(data_files_parallelized_list,dataframes)):\n",
    "        size += round(os.path.getsize(file_)*9.31*1e-10,2)\n",
    "        full_data_frame = full_data_frame.union(df)\n",
    "        print(size)\n",
    "        if i==0:\n",
    "            i += 1\n",
    "            continue\n",
    "        if int(size) >= 60:\n",
    "            starttime = timer()\n",
    "            hashing_data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "            result_hash = hash_tf(hashing_data)\n",
    "            idf_data_hash = idf_generator(result_hash)\n",
    "            time_to_idf = timer() - starttime\n",
    "            time_dict.update({size:time_to_idf})\n",
    "            print(\"Time to HashingTFIDF:\", time_to_idf)\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4f17a-f164-47f1-88d2-1302a5b64b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_folder = \"./metrics_4node\"\n",
    "with open(os.path.join(metric_folder,f\"perf_results_hashingtf_{12}.csv\"),\"a\") as fh:\n",
    "    fh.write(\"Size\"+\",\"+\"Time\"+\"\\n\")\n",
    "    for line in time_mapping:\n",
    "        fh.write(f\"{line[0]}\",+\",\",f\"line{1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ccf62-96f0-4775-b3bc-8fa0b614c369",
   "metadata": {},
   "source": [
    "## Performing time computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566272cc-9e53-4659-9d10-ba2c6879788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyschema = StructType([\n",
    "  StructField('review_body', StringType(), True)\n",
    "  ])\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "full_data_frame = spark.createDataFrame(emptyRDD,emptyschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5441a-7ca7-4f1b-b9ca-a69d14138e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for file_, df in list(zip(data_files_parallelized_list,dataframes)):\n",
    "        size += round(os.path.getsize(file_)*9.31*1e-10,2)\n",
    "        print(size)\n",
    "        full_data_frame = full_data_frame.union(df)\n",
    "        if size >= 0.5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f2b9d-0970-45a6-8ff3-dd3396ae0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = \"review_body\"\n",
    "starttime = timer()\n",
    "hashing_data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "result_hash = hash_tf(hashing_data)\n",
    "idf_data_hash = idf_generator(result_hash)\n",
    "time_to_idf = timer() - starttime\n",
    "print(\"Time to HashingTFIDF:\", timer() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20edb804-1ade-4456-a26c-27c73a2d3fde",
   "metadata": {},
   "source": [
    "## TF IDF with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35911665-38e5-4ac8-8ef0-ee0dc50ac4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyschema = StructType([\n",
    "  StructField('review_body', StringType(), True)\n",
    "  ])\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "full_data_frame = spark.createDataFrame(emptyRDD,emptyschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0d830-7c7a-45c4-8cdf-aa2bf2981716",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = \"review_body\"\n",
    "while True:\n",
    "    size = 0\n",
    "    time_dict_count = {}\n",
    "    i=0\n",
    "    for file_, df in list(zip(data_files_parallelized_list,dataframes)):\n",
    "        size += round(os.path.getsize(file_)*9.31*1e-10,2)\n",
    "        full_data_frame = full_data_frame.union(df)\n",
    "        if int(int(size) % 60) == 0:\n",
    "            starttime = timer()\n",
    "            data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "            result = count_tf(data)\n",
    "            idf_data = idf_generator(result)\n",
    "            time_to_idf = timer() - starttime\n",
    "            time_dict_count.update({size:time_to_idf})\n",
    "            print(\"Time to CountingTFIDF:\", time_to_idf)\n",
    "    break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08201b97-91f5-4982-8b32-43b5ca367981",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_folder = \"./metrics_4node\"\n",
    "with open(os.path.join(metric_folder,f\"perf_results_counttf_{48}.csv\"),\"a\") as fh:\n",
    "    fh.write(\"Size\"+\",\"+\"Time\"+\"\\n\")\n",
    "    for line in sorted(time_dict_count.keys()):\n",
    "        fh.write(f\"{line}\"+\",\"+f\"{time_dict_count[line]}\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a58868-dbdb-4795-99fa-bd0e8dc0207b",
   "metadata": {},
   "source": [
    "## Performing time computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c6b14-2ecb-4888-9721-45223fbaad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyschema = StructType([\n",
    "  StructField('review_body', StringType(), True)\n",
    "  ])\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "full_data_frame = spark.createDataFrame(emptyRDD,emptyschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0becf8-6ae1-4b98-92db-4e756f47b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for file_, df in list(zip(data_files_parallelized_list,dataframes)):\n",
    "        size += round(os.path.getsize(file_)*9.31*1e-10,2)\n",
    "        full_data_frame = full_data_frame.union(df)\n",
    "        if size > 1.6:\n",
    "            print(size)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df596c1-2663-493c-ac78-13d30f281354",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = \"review_body\"\n",
    "starttime = timer()\n",
    "data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "result = count_tf(data)\n",
    "idf_data = idf_generator(result)\n",
    "time_to_idf = timer() - starttime\n",
    "# time_dict_count.update({size:time_to_idf})\n",
    "print(\"Time to CountingTFIDF:\", time_to_idf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
