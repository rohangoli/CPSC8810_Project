{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c8138f-d758-499f-8d75-3bc5dbc00702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8feb688-818a-4cb4-be24-712f2e384245",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"./source.txt\"\n",
    "\n",
    "def list_urls(file):\n",
    "\n",
    "    with open(\"./source.txt\",'r') as fh:\n",
    "        source_files_list = list(line.strip() for line in fh.readlines())\n",
    "    return source_files_list\n",
    "    \n",
    "    \n",
    "def download_dataset(url):\n",
    "    try:\n",
    "        subprocess.call(f\"wget {url}\",shell=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "DATADIR = \"/scratch1/rgoli/aws_customer_reviews\"\n",
    "def unzip_files(DATADIR):\n",
    "    gz_files = os.listdir(DATADIR)\n",
    "    source_files_list = list(line.strip() for line in gz_files)\n",
    "    for file_ in source_files_list:\n",
    "    #     print(file_)\n",
    "        print(subprocess.call([\"gzip\",\"-d\",os.path.join(DATADIR,file_)],shell=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a498068f-e448-427d-a05b-4871e5ef5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files_list = list(filter(lambda file_:file_.endswith(\".tsv\"),os.listdir(DATADIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df3d537-b3c7-4b96-9784-2412c53abe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/software/spackages/linux-centos8-x86_64/gcc-8.3.1/spark-3.1.2-fbpdjvdj5fjlkjnu35rh4aooi2mirndz\n",
      "/home/rgadde/ondemand/data/sys/dashboard/batch_connect/sys/palmetto_jupyter_spark/output/845c3df6-87ad-43ae-99f0-f0eba10dc72f/spark-defaults.conf\n",
      "/software/spackages/linux-centos8-x86_64/gcc-8.3.1/spark-3.1.2-fbpdjvdj5fjlkjnu35rh4aooi2mirndz\n",
      "node2092.palmetto.clemson.edu\n",
      "28240\n",
      "6669\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(os.environ['SPARK_ROOT'])\n",
    "print(os.environ['SPARK_CONFIG_FILE'])\n",
    "print(os.environ['SPARK_ROOT'])\n",
    "print(os.environ['SPARK_MASTER_HOST'])\n",
    "print(os.environ['SPARK_MASTER_PORT'])\n",
    "print(os.environ['SPARK_MASTER_WEBUI_PORT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f51f74-d5dc-463a-adab-8b0001f1cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "PUNCTUATIONS = \"\"\"[!\"#$%&()*+-.\\/\\\\:;<=>?@\\[\\]^_`{|}\\t\\n\\',~â€”]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1253d0-13d8-4ccb-9f16-bd82e4221ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "import gzip\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f5dacb-8c55-45d6-9f05-ec1e5ac9dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f77e1c-b85e-4c33-9d61-ea5298272e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e28367-2968-450d-b623-89e328a37f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from timeit import default_timer as timer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65da31b6-2183-4434-ae44-72d0795ec9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be73788a-8dce-4145-951e-309c4d587099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_gunzip(path):\n",
    "#     with gzip.open(path,'rt') as fh:\n",
    "    data = spark.read.csv(path,sep=\"\\t\")\n",
    "    return data\n",
    "def read_tsv(path):\n",
    "#     print(path)\n",
    "    data = spark.read.csv(path,sep=\"\\t\",header = True)\n",
    "    data = data.select([\"review_body\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4975342-70d9-4e81-8a6d-40725983deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = list(filter(lambda x:x.endswith(\".tsv\"),os.listdir(DATADIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94bd6a79-8f0b-42df-891d-3c7dbb021baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_objects = [file_.split(\"_\")[3] for file_ in data_files]\n",
    "data_files_parallelized = sc.parallelize([os.path.join(DATADIR,file_) for file_ in data_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "272d4e7d-d1c6-44c7-9781-65b115158fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files_parallelized_list = data_files_parallelized.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49196203-51b2-4666-9ab5-751c6a5a9fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read time:  6.939342759549618\n"
     ]
    }
   ],
   "source": [
    "read_t1 = timer()\n",
    "dataframes = list(map(lambda file_:read_tsv(file_),data_files_parallelized_list))\n",
    "read_t2 = timer()\n",
    "print(\"Read time: \",read_t2 - read_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8801ed40-5402-4432-b156-6b14c3c159c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59 0\n",
      "1.66 1\n",
      "1.7 1\n",
      "1.84 1\n",
      "8.08 8\n",
      "9.92 9\n",
      "16.15 4\n",
      "19.17 7\n",
      "22.62 10\n",
      "24.23 0\n",
      "25.12 1\n",
      "26.58 2\n",
      "27.7 3\n",
      "29.53 5\n",
      "31.6 7\n",
      "35.0 11\n",
      "36.2 0\n",
      "37.46 1\n",
      "38.62 2\n",
      "42.53 6\n",
      "44.4 8\n",
      "44.45 8\n",
      "44.89 8\n",
      "44.94 8\n",
      "44.98 8\n",
      "45.29 9\n",
      "48.29 0\n",
      "49.1 1\n",
      "49.57 1\n",
      "50.59 2\n",
      "51.88 3\n",
      "53.0 4\n",
      "55.25 7\n",
      "56.39 8\n",
      "56.62 8\n",
      "56.96 8\n",
      "57.03 9\n",
      "57.76 9\n",
      "59.76 11\n",
      "60.34 0\n",
      "60.72 0\n",
      "64.14 4\n",
      "66.46 6\n",
      "72.7 0\n",
      "72.76 0\n",
      "73.75 1\n"
     ]
    }
   ],
   "source": [
    "size = 0\n",
    "for file_ in data_files:\n",
    "    size += round(os.path.getsize(os.path.join(DATADIR,file_))*9.31*1e-10,2)\n",
    "    print(round(size,2),int(int(size) % 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc88462c-3953-4c38-b4d7-381bc066bdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.866124999999998e-08"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size *9.31*1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ba47cc-52df-4b3a-8b64-45a913e240e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_pre_processing(dataframe,column_name):\n",
    "    dataframe = dataframe.na.drop(subset=[column_name])\n",
    "    dataframe = dataframe.withColumn(column_name,f.lower(f.col(column_name)))\n",
    "    dataframe = dataframe.withColumn(column_name,f.regexp_replace(f.col(column_name), PUNCTUATIONS, ' '))\n",
    "    dataframe = dataframe.withColumn(column_name,f.trim(f.col(column_name)))\n",
    "    tokenizer = Tokenizer(inputCol=column_name, outputCol=\"words\")\n",
    "    dataframe = tokenizer.transform(dataframe)                  \n",
    "    stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=STOPWORDS)\n",
    "    data = stopwordsRemover.transform(dataframe)\n",
    "    return data\n",
    "\n",
    "def hash_tf(dataframe):\n",
    "    starttime = timer()\n",
    "#     tokenizer_hastingtf = Tokenizer(inputCol=\"review_body\", outputCol=\"tokenized\")\n",
    "#     tokenized_data = tokenizer_hastingtf.transform(dataframe)\n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "    result = hashingTF.transform(dataframe)\n",
    "    print(\"Time to Hash\", timer() - starttime)\n",
    "    return result\n",
    "\n",
    "def count_tf(dataframe):\n",
    "    countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\", vocabSize=100000, minDF=5)\n",
    "    model = countVectors.fit(dataframe)\n",
    "    result = model.transform(dataframe)\n",
    "    return result\n",
    "\n",
    "\n",
    "def idf_generator(dataframe):\n",
    "    starttime = timer()\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfmodel = idf.fit(dataframe)\n",
    "    idf_data = idfmodel.transform(dataframe)\n",
    "    print(\"Time to idf\", timer() - starttime)\n",
    "    return idf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed4cbeb8-eb7f-4322-a892-ccbb9d9ff8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyschema = StructType([\n",
    "  StructField('review_body', StringType(), True)\n",
    "  ])\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "full_data_frame = spark.createDataFrame(emptyRDD,emptyschema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239aa4e-93d7-4689-970b-b912983aa0c4",
   "metadata": {},
   "source": [
    "# TF IDF with HasingTF and IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50d344-bd58-4ca8-94c7-054ea684652f",
   "metadata": {},
   "source": [
    "## Performance data size vs Computation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de7f2632-095f-4c99-bd17-9207a017037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59 \n",
      "\n",
      "Time to Hash 0.037309326231479645\n",
      "Time to idf 9.119319986552\n",
      "Time to HashingTFIDF: 9.333043094724417\n",
      "1.6600000000000001 \n",
      "\n",
      "1.7000000000000002 \n",
      "\n",
      "1.8400000000000003 \n",
      "\n",
      "8.08 \n",
      "\n",
      "9.92 \n",
      "\n",
      "16.15 \n",
      "\n",
      "19.169999999999998 \n",
      "\n",
      "22.619999999999997 \n",
      "\n",
      "24.229999999999997 \n",
      "\n",
      "Time to Hash 0.010231640189886093\n",
      "Time to idf 611.530782584101\n",
      "Time to HashingTFIDF: 611.6154892109334\n",
      "25.119999999999997 \n",
      "\n",
      "26.58 \n",
      "\n",
      "27.7 \n",
      "\n",
      "29.53 \n",
      "\n",
      "31.6 \n",
      "\n",
      "35.0 \n",
      "\n",
      "36.2 \n",
      "\n",
      "Time to Hash 0.008726131170988083\n",
      "Time to idf 874.8646063953638\n",
      "Time to HashingTFIDF: 874.9420877359807\n",
      "37.46 \n",
      "\n",
      "38.62 \n",
      "\n",
      "42.53 \n",
      "\n",
      "44.4 \n",
      "\n",
      "44.449999999999996 \n",
      "\n",
      "44.88999999999999 \n",
      "\n",
      "44.93999999999999 \n",
      "\n",
      "44.97999999999999 \n",
      "\n",
      "45.28999999999999 \n",
      "\n",
      "48.28999999999999 \n",
      "\n",
      "Time to Hash 0.007921997457742691\n",
      "Time to idf 1136.538626063615\n",
      "Time to HashingTFIDF: 1136.6134267523885\n",
      "49.099999999999994 \n",
      "\n",
      "49.56999999999999 \n",
      "\n",
      "50.589999999999996 \n",
      "\n",
      "51.879999999999995 \n",
      "\n",
      "52.99999999999999 \n",
      "\n",
      "55.24999999999999 \n",
      "\n",
      "56.38999999999999 \n",
      "\n",
      "56.61999999999999 \n",
      "\n",
      "56.959999999999994 \n",
      "\n",
      "57.029999999999994 \n",
      "\n",
      "57.75999999999999 \n",
      "\n",
      "59.75999999999999 \n",
      "\n",
      "60.33999999999999 \n",
      "\n",
      "Time to Hash 0.007869143038988113\n",
      "Time to idf 1395.81575839594\n",
      "Time to HashingTFIDF: 1395.890726376325\n",
      "60.71999999999999 \n",
      "\n",
      "Time to Hash 0.008065834641456604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rgadde/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/rgadde/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2021.05-5tjen3mrle3pnguoedh4n2stzzsmywn7/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-202d685c648e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mhashing_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf_pre_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_data_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mresult_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashing_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0midf_data_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midf_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mtime_to_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstarttime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtime_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtime_to_idf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-c8ba6fa1a4ef>\u001b[0m in \u001b[0;36midf_generator\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawFeatures\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0midfmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0midf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midfmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time to idf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstarttime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2021.05-5tjen3mrle3pnguoedh4n2stzzsmywn7/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "column_name = \"review_body\"\n",
    "while True:\n",
    "    size = 0\n",
    "    time_dict = {}\n",
    "    i=0\n",
    "    for file_, df in list(zip(data_files_parallelized_list,dataframes)):\n",
    "        size += round(os.path.getsize(file_)*9.31*1e-10,2)\n",
    "        full_data_frame = full_data_frame.union(df)\n",
    "        print(size,\"\\n\")\n",
    "        if int(int(size) % 12) == 0:\n",
    "            starttime = timer()\n",
    "            hashing_data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "            result_hash = hash_tf(hashing_data)\n",
    "            idf_data_hash = idf_generator(result_hash)\n",
    "            time_to_idf = timer() - starttime\n",
    "            time_dict.update({size:time_to_idf})\n",
    "            print(\"Time to HashingTFIDF:\", time_to_idf)\n",
    "             \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a8dfe96-a543-4d81-aef9-632a376963b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_folder = \"./metrics_1node\"\n",
    "with open(os.path.join(metric_folder,f\"perf_results_hashtf_{48}.csv\"),\"a\") as fh:\n",
    "    fh.write(\"Size\"+\",\"+\"Time\"+\"\\n\")\n",
    "    for line in sorted(time_dict.keys()):\n",
    "        fh.write(f\"{line}\"+\",\"+f\"{time_dict[line]}\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ccf62-96f0-4775-b3bc-8fa0b614c369",
   "metadata": {},
   "source": [
    "## Performing time computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78368a47-cae3-4ce2-9c31-7b0e5e13f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyschema = StructType([\n",
    "  StructField('review_body', StringType(), True)\n",
    "  ])\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "full_data_frame = spark.createDataFrame(emptyRDD,emptyschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5441a-7ca7-4f1b-b9ca-a69d14138e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_, df in list(zip(data_files_parallelized_list,dataframes)):\n",
    "        size += round(os.path.getsize(file_)*9.31*1e-10,2)\n",
    "        full_data_frame = full_data_frame.union(df)\n",
    "        if size > 24:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f2b9d-0970-45a6-8ff3-dd3396ae0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "starttime = timer()\n",
    "result_hash = hash_tf(hashing_data)\n",
    "idf_data_hash = idf_generator(result_hash)\n",
    "print(\"Time to HashingTFIDF:\", timer() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20edb804-1ade-4456-a26c-27c73a2d3fde",
   "metadata": {},
   "source": [
    "## TF IDF with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0d830-7c7a-45c4-8cdf-aa2bf2981716",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = \"review_body\"\n",
    "while True:\n",
    "    size = 0\n",
    "    time_dict_count = {}\n",
    "    i=0\n",
    "    for file_, df in list(zip(data_files_parallelized_list,dataframes)):\n",
    "        size += round(os.path.getsize(file_)*9.31*1e-10,2)\n",
    "        full_data_frame = full_data_frame.union(df)\n",
    "        if int(int(size) % 12) == 0:\n",
    "            starttime = timer()\n",
    "            data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "            result = count_tf(data)\n",
    "            idf_data = idf_generator(result)\n",
    "            time_to_idf = timer() - starttime\n",
    "            time_dict_count.update({size:time_to_idf})\n",
    "            print(\"Time to CountTFIDF:\", time_to_idf)\n",
    "        if i==5:\n",
    "            break\n",
    "        i+=1\n",
    "    break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08201b97-91f5-4982-8b32-43b5ca367981",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_folder = \"./metrics_1node\"\n",
    "with open(os.path.join(metric_folder,f\"perf_results_counttf_{48}.csv\"),\"a\") as fh:\n",
    "    fh.write(\"Size\"+\",\"+\"Time\"+\"\\n\")\n",
    "    for line in sorted(time_dict_count.keys()):\n",
    "        fh.write(f\"{line}\"+\",\"+f\"{time_dict_count[line]}\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a58868-dbdb-4795-99fa-bd0e8dc0207b",
   "metadata": {},
   "source": [
    "## Performing time computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c6b14-2ecb-4888-9721-45223fbaad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyschema = StructType([\n",
    "  StructField('review_body', StringType(), True)\n",
    "  ])\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "full_data_frame = spark.createDataFrame(emptyRDD,emptyschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0becf8-6ae1-4b98-92db-4e756f47b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_, df in list(zip(data_files_parallelized_list,dataframes)):\n",
    "        size += round(os.path.getsize(file_)*9.31*1e-10,2)\n",
    "        full_data_frame = full_data_frame.union(df)\n",
    "        if size > 24:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df596c1-2663-493c-ac78-13d30f281354",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "data = tf_idf_pre_processing(full_data_frame,column_name)\n",
    "result = count_tf(data)\n",
    "idf_data = idf_generator(result)\n",
    "print(\"Time to CountTFIDF:\", timer() - starttime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
